{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pranavpsv/gpt2-genre-story-generator\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"pranavpsv/gpt2-genre-story-generator\", return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_logits(sentence):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(sentence,add_special_tokens=True, padding=True)\n",
    "        outputs = model(torch.tensor(inputs['input_ids']))\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits[-1, :]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_from_logits(idx, logits):\n",
    "    prob = logits[idx]\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_id_form_dict(word):\n",
    "    return tokenizer.get_vocab()['Ġ'+ word]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDict(target_id, sent, logits, k, d, last_id):\n",
    "    if k<=0:\n",
    "        return\n",
    "    \n",
    "    _, topk_ids = torch.topk(logits, 3)\n",
    "    for i in range(len(topk_ids)):\n",
    "        new_word = tokenizer.decode([topk_ids[i]])\n",
    "        diff = get_prob_from_logits(topk_ids[i], logits) - get_prob_from_logits(target_id, logits)\n",
    "        new_sent = sent + new_word\n",
    "        new_logits = get_next_logits(new_sent)\n",
    "        d[(k,topk_ids[i],last_id)] = diff\n",
    "        buildDict(target_id, new_sent, new_logits, k-1, d, topk_ids[i])\n",
    "    return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_list(d, k):\n",
    "    action = []\n",
    "    min_v = min(d.values())\n",
    "    find = list(d.keys())[list(d.values()).index(min_v)]\n",
    "    i = find[0]\n",
    "\n",
    "    action.append(find[1])\n",
    "    while i<k:\n",
    "        for key in d.keys():\n",
    "            if (find[2] == key[1]) and (key[0] == i+1):\n",
    "                find = key\n",
    "                i = find[0]\n",
    "                action.append(find[1])\n",
    "                \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_sentence_action_search(sent, target_word, tolerant):\n",
    "\n",
    "    action = []\n",
    "    results = []\n",
    "    for i in range(20):\n",
    "        #print('sent:', sent)\n",
    "\n",
    "        if len(action)==0:\n",
    "            logits = get_next_logits(sent)\n",
    "\n",
    "            target_id = get_word_id_form_dict(target_word)\n",
    "            target_prob = get_prob_from_logits(target_id, logits)\n",
    "            #print(logits.shape)\n",
    "            #print('target: ', target_prob)\n",
    "            d = dict()\n",
    "            k = 3\n",
    "            buildDict(target_id, sent, logits, k, d, \"\")\n",
    "\n",
    "            action = get_action_list(d, k)\n",
    "\n",
    "        # next_id = topk_ids[random.randint(0,9)]\n",
    "        next_id = action.pop()\n",
    "#         print('next_id: ', next_id)\n",
    "        next_word = tokenizer.decode([next_id])\n",
    "#         print('next_word: ', next_word)\n",
    "#         print('argmax: ',get_prob_from_logits(next_id, logits))\n",
    "        diff = get_prob_from_logits(next_id, logits) - target_prob\n",
    "        #print('diff: ',diff)\n",
    "        if diff < tolerant:\n",
    "            temp_word = sent + ' ' + target_word\n",
    "            results.append(temp_word)\n",
    "            tolerant = 0.3\n",
    "        else:\n",
    "            tolerant += 0.1\n",
    "        for i in range(len(results)):\n",
    "            results[i] += next_word\n",
    "        sent += next_word\n",
    "#         print('tolerant:', tolerant)\n",
    "#         print('--------------------------------------------') \n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<BOS> <thriller> Long long ago, the world of the martial artists and martial-clerk were ruled by pig Master Li and Master Wu']\n"
     ]
    }
   ],
   "source": [
    "genre_list = ['superhero', 'action', 'drama', 'horror', 'thriller', 'sci_fi']\n",
    "sent = '<BOS> <'+genre_list[random.randint(0,len(genre_list)-1)] +'> Long long ago'\n",
    "target_word = 'pig'\n",
    "tolerant = 0.3\n",
    "generate_sentence_action_search(sent, target_word, tolerant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_topk_search(input_word, target_word, tolerant, genre, total_loop):\n",
    "    sent = '<BOS> <'+genre +'>' + input_word\n",
    "    current_word = input_word\n",
    "    results = dict()\n",
    "    for i in range( total_loop):\n",
    "        #print('sent:', sent)\n",
    "        logits = get_next_logits(sent)\n",
    "\n",
    "        target_id = get_word_id_form_dict(target_word)\n",
    "        target_prob = get_prob_from_logits(target_id, logits)\n",
    "        #print(logits.shape)\n",
    "        #print('target: ', target_prob)\n",
    "        _, topk_ids = torch.topk(logits, 10)\n",
    "        next_id = topk_ids[random.randint(0,9)]\n",
    "#         print('next_id: ', next_id)\n",
    "        next_word = tokenizer.decode([next_id])\n",
    "#         print('next_word: ', next_word)\n",
    "#         print('argmax: ',get_prob_from_logits(next_id, logits))\n",
    "        diff = get_prob_from_logits(next_id, logits) - target_prob\n",
    "        #print('diff: ',diff)\n",
    "        if diff < tolerant:\n",
    "            temp_word = current_word + ' ' + target_word\n",
    "            results[generate_sentence_topk_search(temp_word, target_word, float('-inf'), genre, total_loop-i-1)] = diff\n",
    "            tolerant -= 1\n",
    "        else:\n",
    "            tolerant +=0.1\n",
    "        current_word += next_word\n",
    "        sent += next_word\n",
    "#         print('tolerant:', tolerant)\n",
    "#         print('--------------------------------------------') \n",
    "    if total_loop == TOTAL_LOOP:\n",
    "        #print(results)\n",
    "        if len(results)>0:\n",
    "            min_diff = min(results.values())\n",
    "            print(list(results.keys())[list(results.values()).index(min_diff)])\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> <sci_fi>Long long ago there came in a small town an abandoned settlement called Sengun who have developed advanced weaponry to fend a wave. They were attacked with the use an unusual device called water cannons they have a unique type\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<BOS> <sci_fi>Long long ago there came in a small town an abandoned settlement called Sengun who have developed advanced weaponry to fend a wave. They were attacked with the use an unusual device called Airtan. But it was'"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOTAL_LOOP = 40\n",
    "genre_list = ['superhero', 'action', 'drama', 'horror', 'thriller', 'sci_fi']\n",
    "genre = genre_list[random.randint(0,len(genre_list)-1)]\n",
    "input_word = 'Long long ago'\n",
    "target_word = 'water'\n",
    "tolerant = 10\n",
    "generate_sentence_topk_search(input_word, target_word, tolerant, genre, TOTAL_LOOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_nucleus(input_word, target_word, tolerant, genre, total_loop):\n",
    "    sent = '<BOS> <'+genre +'>' + input_word\n",
    "    current_word = input_word\n",
    "    results = dict()\n",
    "    for i in range( total_loop):\n",
    "        logits = get_next_logits(sent)\n",
    "\n",
    "        target_id = get_word_id_form_dict(target_word)\n",
    "        target_prob = get_prob_from_logits(target_id, logits)\n",
    "\n",
    "        filtered_logits = top_k_top_p_filtering(logits, top_k=0, top_p=0.9)\n",
    "        probabilities = F.softmax(filtered_logits, dim=-1)\n",
    "        next_id = torch.multinomial(probabilities, 1)\n",
    "\n",
    "        next_word = tokenizer.decode([next_id])\n",
    "\n",
    "        diff = get_prob_from_logits(next_id, logits) - target_prob\n",
    "        if diff < tolerant:\n",
    "            temp_word = current_word + ' ' + target_word\n",
    "            results[generate_sentence_topk_search(temp_word, target_word, float('-inf'), genre, total_loop-i-1)] = diff\n",
    "            tolerant -= 1\n",
    "        else:\n",
    "            tolerant +=0.1\n",
    "        current_word += next_word\n",
    "        sent += next_word\n",
    "#         print('tolerant:', tolerant)\n",
    "#         print('--------------------------------------------') \n",
    "    if total_loop == TOTAL_LOOP:\n",
    "        #print(results)\n",
    "        if len(results)>0:\n",
    "            min_diff = min(results.values())\n",
    "            print(list(results.keys())[list(results.values()).index(min_diff)])\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<BOS> <thriller>Long long ago, a group of Eonians, led by Gokhō, was dispatched to the Moon, but their arrival caused chaos amongst the others. Still a distant memory, most Eonians decided'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOTAL_LOOP = 40\n",
    "genre_list = ['superhero', 'action', 'drama', 'horror', 'thriller', 'sci_fi']\n",
    "genre = genre_list[random.randint(0,len(genre_list)-1)]\n",
    "input_word = 'Long long ago'\n",
    "target_word = 'water'\n",
    "tolerant = 10\n",
    "generate_sentence_nucleus(input_word, target_word, tolerant, genre, TOTAL_LOOP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of progressive rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'pig', 'dirt', style = \"horror\"\n",
    "\n",
    "In the deep dark woods grass where we hear that many dead pig men come in dirt on trees that slope inwards. They kill their horses by hanging. Then the men enter their hideaway where they hang and kill two pig brothers by tying their hands and neck in a string attached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
